{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon- Sentimental Analysis(Binary).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl2fmE7SuqBs"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dst-yKi4vETy"
      },
      "source": [
        "df=pd.read_csv('review.csv') # name of the csv file is 'review.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO87hgGpvrnJ"
      },
      "source": [
        "text = df['text'] #extracting the reviews\n",
        "label = df['score'] #extracting the ratings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KFs77TE2OkD",
        "outputId": "142b7a98-55d8-49e0-ef71-3b4e3395bec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "text[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'edit after months of using this shoe i have decided to reduce my rating to stars i still like the shoes they fit great and grip fantastically however this grip i believe is provided in a trade off for durability the rubber while super grippy is soft and deteriorates rather quickly i spoke with a rep at rei about madrock shoes my wear issue and she said that madrock uses thinnercheaper rubber to sell at lower prices she said that most people who buy madrock have the intention of resoling with better rubber once they are broken in i would still recommend these shoes to anyone who is starting out but keep in mind they will wear down quickly end edit ive been climbing indoor for about two months and after getting acquainted with my abilities through rental shoes i decided i really needed a pair of my own to push routes i had been struggling on i like these for the following reasons comfort i tried on a ton of shoes in person to make sure i knew what i was getting it was a toss up between these and thela sportiva nago mens climbing shoe both provided a great fit however i felt that the mad rock phoenixs had less of a dead toe feel the being said these have a great toe box and firm rand enough to provide plenty of comfort when edging on tiny holds these also grip my heels extremely well so heel hooks and the like arent a problem i remove them during long rests but they are comfortable enough to leave on for extended periods of timefit street shoes i wear an and i like my climbing shoes tight not uncomfortably tight but tolerable i was wearing a in the rental evolv defys at my gym however i was surprised that i had to size down to a in these to get the feel i was looking for since these arent synthetic uppers i fully expect them to stretch a bit but as of now they are nice and tight the laceups really provide a wide range of fit and adjustability i figure that the best way to find a climbing shoe is to first try it on in person develop a relationship with that shoe and then for any future buys order it onlineperformance this is where having your own shoes really pays off before i got these i was doing and some routes after getting these i redid some of the routes that i struggled on before and practically ran up them im now completing bc routes and theres a im excited to work these shoes have a slight downturn which provides great tension for sporty overhang routes and enough edge to stand on tiny holds firmly the toe is thin enough to jam into finger cracks and the heel has a ridged grip which really grabs holds my dyno has increased roughly inches with these and i can slab up slippery faces pretty easily now the box has some label about sciencefriction rubber on the soles and i can attest to its abilitycare so far i havent had to do anything to these to prevent them from smelling as with any shoe you want to let them air out after each climb away from direct sun light additionally i have been climbing about days a week in these for almost weeks now and they dont show a sign of wear i imagine once i venture outdoors i hear austin tx climbing has it all great limestone deepwater soloing and pink granite a bit west they might start to wear but until then they seem well constructedoverall i would say this is a great shoe for indoor climbing having not ventured onto any real rock yet i cant speak toward its performance however i would imagine that it would function just fine on real rock as it does on whatever those indoor gyms are molded out of'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42rUmBoFwdPU",
        "outputId": "0b44aa53-ac31-4907-a8dc-db412061b80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3G9UEH8wfuh"
      },
      "source": [
        "#Tokenize â€” Create Vocab to Int mapping dictionary\n",
        "from collections import Counter\n",
        "all_text2 = ' '.join(text)\n",
        "# create a list of words\n",
        "words = all_text2.split()\n",
        "# Count all the words using Counter Method\n",
        "count_words = Counter(words)\n",
        "\n",
        "total_words = len(words)\n",
        "sorted_words = count_words.most_common(total_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Puw2uUah1jgZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h20lwXK6w29j"
      },
      "source": [
        "#In order to create a vocab to int mapping dictionary\n",
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACcD_izF197v",
        "outputId": "f2bcb95e-15f8-4735-ffbf-fce7ecc9fc4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vocab_to_int"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'i': 2,\n",
              " 'and': 3,\n",
              " 'a': 4,\n",
              " 'to': 5,\n",
              " 'they': 6,\n",
              " 'for': 7,\n",
              " 'of': 8,\n",
              " 'are': 9,\n",
              " 'them': 10,\n",
              " 'these': 11,\n",
              " 'my': 12,\n",
              " 'in': 13,\n",
              " 'is': 14,\n",
              " 'shoes': 15,\n",
              " 'have': 16,\n",
              " 'on': 17,\n",
              " 'it': 18,\n",
              " 'that': 19,\n",
              " 'but': 20,\n",
              " 'this': 21,\n",
              " 'with': 22,\n",
              " 'very': 23,\n",
              " 'was': 24,\n",
              " 'shoe': 25,\n",
              " 'not': 26,\n",
              " 'comfortable': 27,\n",
              " 'so': 28,\n",
              " 'pair': 29,\n",
              " 'as': 30,\n",
              " 'you': 31,\n",
              " 'size': 32,\n",
              " 'wear': 33,\n",
              " 'great': 34,\n",
              " 'be': 35,\n",
              " 'boots': 36,\n",
              " 'feet': 37,\n",
              " 'like': 38,\n",
              " 'had': 39,\n",
              " 'were': 40,\n",
              " 'at': 41,\n",
              " 'just': 42,\n",
              " 'good': 43,\n",
              " 'all': 44,\n",
              " 'would': 45,\n",
              " 'fit': 46,\n",
              " 'or': 47,\n",
              " 'bought': 48,\n",
              " 'when': 49,\n",
              " 'if': 50,\n",
              " 'well': 51,\n",
              " 'love': 52,\n",
              " 'will': 53,\n",
              " 'out': 54,\n",
              " 'me': 55,\n",
              " 'than': 56,\n",
              " 'get': 57,\n",
              " 'from': 58,\n",
              " 'more': 59,\n",
              " 'look': 60,\n",
              " 'too': 61,\n",
              " 'up': 62,\n",
              " 'one': 63,\n",
              " 'can': 64,\n",
              " 'time': 65,\n",
              " 'only': 66,\n",
              " 'am': 67,\n",
              " 'about': 68,\n",
              " 'your': 69,\n",
              " 'because': 70,\n",
              " 'really': 71,\n",
              " 'buy': 72,\n",
              " 'little': 73,\n",
              " 'after': 74,\n",
              " 'price': 75,\n",
              " 'years': 76,\n",
              " 'dont': 77,\n",
              " 'other': 78,\n",
              " 'no': 79,\n",
              " 'day': 80,\n",
              " 'first': 81,\n",
              " 'do': 82,\n",
              " 'he': 83,\n",
              " 'foot': 84,\n",
              " 'its': 85,\n",
              " 'ive': 86,\n",
              " 'has': 87,\n",
              " 'made': 88,\n",
              " 'an': 89,\n",
              " 'ordered': 90,\n",
              " 'got': 91,\n",
              " 'quality': 92,\n",
              " 'im': 93,\n",
              " 'also': 94,\n",
              " 'wearing': 95,\n",
              " 'been': 96,\n",
              " 'even': 97,\n",
              " 'leather': 98,\n",
              " 'much': 99,\n",
              " 'work': 100,\n",
              " 'what': 101,\n",
              " 'now': 102,\n",
              " 'nice': 103,\n",
              " 'still': 104,\n",
              " 'boot': 105,\n",
              " 'which': 106,\n",
              " 'some': 107,\n",
              " 'most': 108,\n",
              " 'recommend': 109,\n",
              " 'worn': 110,\n",
              " 'off': 111,\n",
              " 'looking': 112,\n",
              " 'back': 113,\n",
              " 'feel': 114,\n",
              " 'long': 115,\n",
              " 'walking': 116,\n",
              " 'find': 117,\n",
              " 'perfect': 118,\n",
              " 'go': 119,\n",
              " 'any': 120,\n",
              " 'new': 121,\n",
              " 'around': 122,\n",
              " 'last': 123,\n",
              " 'amazon': 124,\n",
              " 'slippers': 125,\n",
              " 'she': 126,\n",
              " 'color': 127,\n",
              " 'bit': 128,\n",
              " 'sandals': 129,\n",
              " 'over': 130,\n",
              " 'sole': 131,\n",
              " 'use': 132,\n",
              " 'right': 133,\n",
              " 'another': 134,\n",
              " 'wide': 135,\n",
              " 'purchased': 136,\n",
              " 'product': 137,\n",
              " 'there': 138,\n",
              " 'we': 139,\n",
              " 'could': 140,\n",
              " 'did': 141,\n",
              " 'ever': 142,\n",
              " 'order': 143,\n",
              " 'heel': 144,\n",
              " 'best': 145,\n",
              " 'support': 146,\n",
              " 'by': 147,\n",
              " 'better': 148,\n",
              " 'theyre': 149,\n",
              " 'black': 150,\n",
              " 'never': 151,\n",
              " 'since': 152,\n",
              " 'two': 153,\n",
              " 'put': 154,\n",
              " 'big': 155,\n",
              " 'every': 156,\n",
              " 'small': 157,\n",
              " 'same': 158,\n",
              " 'walk': 159,\n",
              " 'run': 160,\n",
              " 'style': 161,\n",
              " 'year': 162,\n",
              " 'again': 163,\n",
              " 'think': 164,\n",
              " 'wore': 165,\n",
              " 'make': 166,\n",
              " 'enough': 167,\n",
              " 'easy': 168,\n",
              " 'comfort': 169,\n",
              " 'warm': 170,\n",
              " 'need': 171,\n",
              " 'how': 172,\n",
              " 'pairs': 173,\n",
              " 'toe': 174,\n",
              " 'always': 175,\n",
              " 'before': 176,\n",
              " 'then': 177,\n",
              " 'found': 178,\n",
              " 'cute': 179,\n",
              " 'true': 180,\n",
              " 'way': 181,\n",
              " 'their': 182,\n",
              " 'purchase': 183,\n",
              " 'few': 184,\n",
              " 'high': 185,\n",
              " 'didnt': 186,\n",
              " 'happy': 187,\n",
              " 'keep': 188,\n",
              " 'old': 189,\n",
              " 'many': 190,\n",
              " 'used': 191,\n",
              " 'her': 192,\n",
              " 'cant': 193,\n",
              " 'hard': 194,\n",
              " 'however': 195,\n",
              " 'while': 196,\n",
              " 'though': 197,\n",
              " 'lot': 198,\n",
              " 'loves': 199,\n",
              " 'who': 200,\n",
              " 'looks': 201,\n",
              " 'his': 202,\n",
              " 'pretty': 203,\n",
              " 'soft': 204,\n",
              " 'ones': 205,\n",
              " 'down': 206,\n",
              " 'without': 207,\n",
              " 'want': 208,\n",
              " 'say': 209,\n",
              " 'narrow': 210,\n",
              " 'know': 211,\n",
              " 'going': 212,\n",
              " 'problem': 213,\n",
              " 'take': 214,\n",
              " 'days': 215,\n",
              " 'socks': 216,\n",
              " 'months': 217,\n",
              " 'half': 218,\n",
              " 'soles': 219,\n",
              " 'different': 220,\n",
              " 'tried': 221,\n",
              " 'into': 222,\n",
              " 'through': 223,\n",
              " 'second': 224,\n",
              " 'worth': 225,\n",
              " 'received': 226,\n",
              " 'sure': 227,\n",
              " 'buying': 228,\n",
              " 'husband': 229,\n",
              " 'definitely': 230,\n",
              " 'return': 231,\n",
              " 'comfy': 232,\n",
              " 'toes': 233,\n",
              " 'wanted': 234,\n",
              " 'highly': 235,\n",
              " 'both': 236,\n",
              " 'where': 237,\n",
              " 'should': 238,\n",
              " 'brand': 239,\n",
              " 'almost': 240,\n",
              " 'try': 241,\n",
              " 'light': 242,\n",
              " 'store': 243,\n",
              " 'give': 244,\n",
              " 'excellent': 245,\n",
              " 'several': 246,\n",
              " 'thought': 247,\n",
              " 'top': 248,\n",
              " 'far': 249,\n",
              " 'came': 250,\n",
              " 'running': 251,\n",
              " 'quite': 252,\n",
              " 'owned': 253,\n",
              " 'getting': 254,\n",
              " 'box': 255,\n",
              " 'ago': 256,\n",
              " 'seem': 257,\n",
              " 'arch': 258,\n",
              " 'being': 259,\n",
              " 'usually': 260,\n",
              " 'tight': 261,\n",
              " 'son': 262,\n",
              " 'thing': 263,\n",
              " 'durable': 264,\n",
              " 'daughter': 265,\n",
              " 'money': 266,\n",
              " 'flip': 267,\n",
              " 'arrived': 268,\n",
              " 'larger': 269,\n",
              " 'bag': 270,\n",
              " 'extremely': 271,\n",
              " 'inside': 272,\n",
              " 'something': 273,\n",
              " 'come': 274,\n",
              " 'does': 275,\n",
              " 'perfectly': 276,\n",
              " 'makes': 277,\n",
              " 'actually': 278,\n",
              " 'shipping': 279,\n",
              " 'said': 280,\n",
              " 'wears': 281,\n",
              " 'colors': 282,\n",
              " 'break': 283,\n",
              " 'see': 284,\n",
              " 'loved': 285,\n",
              " 'needed': 286,\n",
              " 'smaller': 287,\n",
              " 'brown': 288,\n",
              " 'slip': 289,\n",
              " 'reviews': 290,\n",
              " 'may': 291,\n",
              " 'those': 292,\n",
              " 'sandal': 293,\n",
              " 'fine': 294,\n",
              " 'casual': 295,\n",
              " 'dress': 296,\n",
              " 'width': 297,\n",
              " 'probably': 298,\n",
              " 'jeans': 299,\n",
              " 'super': 300,\n",
              " 'stylish': 301,\n",
              " 'fits': 302,\n",
              " 'hours': 303,\n",
              " 'doesnt': 304,\n",
              " 'once': 305,\n",
              " 'own': 306,\n",
              " 'week': 307,\n",
              " 'yet': 308,\n",
              " 'went': 309,\n",
              " 'pleased': 310,\n",
              " 'less': 311,\n",
              " 'looked': 312,\n",
              " 'winter': 313,\n",
              " 'rubber': 314,\n",
              " 'summer': 315,\n",
              " 'water': 316,\n",
              " 'bad': 317,\n",
              " 'sneakers': 318,\n",
              " 'id': 319,\n",
              " 'flops': 320,\n",
              " 'large': 321,\n",
              " 'wish': 322,\n",
              " 'sizes': 323,\n",
              " 'quickly': 324,\n",
              " 'here': 325,\n",
              " 'house': 326,\n",
              " 'strap': 327,\n",
              " 'couple': 328,\n",
              " 'took': 329,\n",
              " 'able': 330,\n",
              " 'anyone': 331,\n",
              " 'times': 332,\n",
              " 'felt': 333,\n",
              " 'heels': 334,\n",
              " 'anything': 335,\n",
              " 'christmas': 336,\n",
              " 'exactly': 337,\n",
              " 'dry': 338,\n",
              " 'wet': 339,\n",
              " 'online': 340,\n",
              " 'least': 341,\n",
              " 'bottom': 342,\n",
              " 'snow': 343,\n",
              " 'absolutely': 344,\n",
              " 'disappointed': 345,\n",
              " 'seems': 346,\n",
              " 'side': 347,\n",
              " 'people': 348,\n",
              " 'white': 349,\n",
              " 'crocs': 350,\n",
              " 'longer': 351,\n",
              " 'wont': 352,\n",
              " 'outside': 353,\n",
              " 'normally': 354,\n",
              " 'next': 355,\n",
              " 'everything': 356,\n",
              " 'expected': 357,\n",
              " 'thats': 358,\n",
              " 'sizing': 359,\n",
              " 'having': 360,\n",
              " 'gift': 361,\n",
              " 'especially': 362,\n",
              " 'fast': 363,\n",
              " 'easily': 364,\n",
              " 'hurt': 365,\n",
              " 'might': 366,\n",
              " 'problems': 367,\n",
              " 'material': 368,\n",
              " 'compliments': 369,\n",
              " 'ill': 370,\n",
              " 'ankle': 371,\n",
              " 'laces': 372,\n",
              " 'ordering': 373,\n",
              " 'him': 374,\n",
              " 'uncomfortable': 375,\n",
              " 'part': 376,\n",
              " 'couldnt': 377,\n",
              " 'heavy': 378,\n",
              " 'cheap': 379,\n",
              " 'weeks': 380,\n",
              " 'three': 381,\n",
              " 'hold': 382,\n",
              " 'extra': 383,\n",
              " 'away': 384,\n",
              " 'sturdy': 385,\n",
              " 'decided': 386,\n",
              " 'item': 387,\n",
              " 'our': 388,\n",
              " 'real': 389,\n",
              " 'everyday': 390,\n",
              " 'pain': 391,\n",
              " 'wrong': 392,\n",
              " 'regular': 393,\n",
              " 'slipper': 394,\n",
              " 'overall': 395,\n",
              " 'service': 396,\n",
              " 'bigger': 397,\n",
              " 'normal': 398,\n",
              " 'started': 399,\n",
              " 'until': 400,\n",
              " 'hiking': 401,\n",
              " 'end': 402,\n",
              " 'design': 403,\n",
              " 'short': 404,\n",
              " 'favorite': 405,\n",
              " 'straps': 406,\n",
              " 'deal': 407,\n",
              " 'maybe': 408,\n",
              " 'youre': 409,\n",
              " 'such': 410,\n",
              " 'why': 411,\n",
              " 'stars': 412,\n",
              " 'finally': 413,\n",
              " 'things': 414,\n",
              " 'although': 415,\n",
              " 'mine': 416,\n",
              " 'wonderful': 417,\n",
              " 's': 418,\n",
              " 'picture': 419,\n",
              " 'already': 420,\n",
              " 'beautiful': 421,\n",
              " 'says': 422,\n",
              " 'awesome': 423,\n",
              " 'expensive': 424,\n",
              " 'front': 425,\n",
              " 'returned': 426,\n",
              " 'fact': 427,\n",
              " 'kind': 428,\n",
              " 'gave': 429,\n",
              " 'glad': 430,\n",
              " 'room': 431,\n",
              " 'month': 432,\n",
              " 'left': 433,\n",
              " 'using': 434,\n",
              " 'must': 435,\n",
              " 'liked': 436,\n",
              " 'mens': 437,\n",
              " 'feels': 438,\n",
              " 'company': 439,\n",
              " 'cold': 440,\n",
              " 'past': 441,\n",
              " 'us': 442,\n",
              " 'pay': 443,\n",
              " 'thick': 444,\n",
              " 'stay': 445,\n",
              " 'between': 446,\n",
              " 'full': 447,\n",
              " 'miles': 448,\n",
              " 'stretch': 449,\n",
              " 'nothing': 450,\n",
              " 'weight': 451,\n",
              " 'review': 452,\n",
              " 'thanks': 453,\n",
              " 'recommended': 454,\n",
              " 'wouldnt': 455,\n",
              " 'plus': 456,\n",
              " 'weather': 457,\n",
              " 'others': 458,\n",
              " 'trying': 459,\n",
              " 'slightly': 460,\n",
              " 'works': 461,\n",
              " 'apart': 462,\n",
              " 'live': 463,\n",
              " 'clarks': 464,\n",
              " 'else': 465,\n",
              " 'snug': 466,\n",
              " 'home': 467,\n",
              " 'area': 468,\n",
              " 'lasted': 469,\n",
              " 'havent': 470,\n",
              " 'type': 471,\n",
              " 'hope': 472,\n",
              " 'classic': 473,\n",
              " 'stiff': 474,\n",
              " 'making': 475,\n",
              " 'instead': 476,\n",
              " 'under': 477,\n",
              " 'cost': 478,\n",
              " 'thin': 479,\n",
              " 'myself': 480,\n",
              " 'sent': 481,\n",
              " 'available': 482,\n",
              " 'wife': 483,\n",
              " 'replace': 484,\n",
              " 'reason': 485,\n",
              " 'wasnt': 486,\n",
              " 'each': 487,\n",
              " 'womens': 488,\n",
              " 'flat': 489,\n",
              " 'due': 490,\n",
              " 'during': 491,\n",
              " 'clean': 492,\n",
              " 'tell': 493,\n",
              " 'model': 494,\n",
              " 'arent': 495,\n",
              " 'hot': 496,\n",
              " 'either': 497,\n",
              " 'daily': 498,\n",
              " 'paid': 499,\n",
              " 'cool': 500,\n",
              " 'loose': 501,\n",
              " 'expect': 502,\n",
              " 'suede': 503,\n",
              " 'stores': 504,\n",
              " 'saw': 505,\n",
              " 'waterproof': 506,\n",
              " 'uggs': 507,\n",
              " 'red': 508,\n",
              " 'shape': 509,\n",
              " 'read': 510,\n",
              " 'believe': 511,\n",
              " 'job': 512,\n",
              " 'fall': 513,\n",
              " 'amazing': 514,\n",
              " 'value': 515,\n",
              " 'place': 516,\n",
              " 'thank': 517,\n",
              " 'isnt': 518,\n",
              " 'let': 519,\n",
              " 'sale': 520,\n",
              " 'help': 521,\n",
              " 'm': 522,\n",
              " 'rather': 523,\n",
              " 'traction': 524,\n",
              " 'low': 525,\n",
              " 'quick': 526,\n",
              " 'goes': 527,\n",
              " 'blisters': 528,\n",
              " 'within': 529,\n",
              " 'etc': 530,\n",
              " 'tennis': 531,\n",
              " 'seller': 532,\n",
              " 'unfortunately': 533,\n",
              " 'line': 534,\n",
              " 'plastic': 535,\n",
              " 'person': 536,\n",
              " 'broken': 537,\n",
              " 'trip': 538,\n",
              " 'ok': 539,\n",
              " 'issue': 540,\n",
              " 'third': 541,\n",
              " 'completely': 542,\n",
              " 'insole': 543,\n",
              " 'wider': 544,\n",
              " 'soon': 545,\n",
              " 'lots': 546,\n",
              " 'everyone': 547,\n",
              " 'satisfied': 548,\n",
              " 'delivery': 549,\n",
              " 'guess': 550,\n",
              " 'runs': 551,\n",
              " 'lightweight': 552,\n",
              " 'cannot': 553,\n",
              " 'medium': 554,\n",
              " 'someone': 555,\n",
              " 'otherwise': 556,\n",
              " 'school': 557,\n",
              " 'original': 558,\n",
              " 'difficult': 559,\n",
              " 'broke': 560,\n",
              " 'height': 561,\n",
              " 'gotten': 562,\n",
              " 'show': 563,\n",
              " 'blue': 564,\n",
              " 'comes': 565,\n",
              " 'pants': 566,\n",
              " 'whole': 567,\n",
              " 'sneaker': 568,\n",
              " 'often': 569,\n",
              " 'shipped': 570,\n",
              " 'life': 571,\n",
              " 'brands': 572,\n",
              " 'ice': 573,\n",
              " 'customer': 574,\n",
              " 'held': 575,\n",
              " 'previous': 576,\n",
              " 'breaking': 577,\n",
              " 'similar': 578,\n",
              " 'local': 579,\n",
              " 'cushioning': 580,\n",
              " 'kept': 581,\n",
              " 'case': 582,\n",
              " 'lining': 583,\n",
              " 'stand': 584,\n",
              " 'provide': 585,\n",
              " 'today': 586,\n",
              " 'cheaper': 587,\n",
              " 'finding': 588,\n",
              " 'insoles': 589,\n",
              " 'recently': 590,\n",
              " 'rain': 591,\n",
              " 'upper': 592,\n",
              " 'period': 593,\n",
              " 'worked': 594,\n",
              " 'cushion': 595,\n",
              " 'experience': 596,\n",
              " 'needs': 597,\n",
              " 'start': 598,\n",
              " 'fantastic': 599,\n",
              " 'adidas': 600,\n",
              " 'across': 601,\n",
              " 'send': 602,\n",
              " 'cut': 603,\n",
              " 'care': 604,\n",
              " 'gets': 605,\n",
              " 'wallet': 606,\n",
              " 'purchasing': 607,\n",
              " 'likes': 608,\n",
              " 'length': 609,\n",
              " 'noticed': 610,\n",
              " 'shopping': 611,\n",
              " 'construction': 612,\n",
              " 'friend': 613,\n",
              " 'nicely': 614,\n",
              " 'velcro': 615,\n",
              " 'working': 616,\n",
              " 'free': 617,\n",
              " 'office': 618,\n",
              " 'kids': 619,\n",
              " 'ended': 620,\n",
              " 'solid': 621,\n",
              " 'balance': 622,\n",
              " 'coming': 623,\n",
              " 'point': 624,\n",
              " 'sexy': 625,\n",
              " 'seemed': 626,\n",
              " 'later': 627,\n",
              " 'seen': 628,\n",
              " 'business': 629,\n",
              " 'tend': 630,\n",
              " 'condition': 631,\n",
              " 'sometimes': 632,\n",
              " 'given': 633,\n",
              " 'padding': 634,\n",
              " 'rockport': 635,\n",
              " 'youll': 636,\n",
              " 'usual': 637,\n",
              " 'plan': 638,\n",
              " 'walked': 639,\n",
              " 'told': 640,\n",
              " 'wait': 641,\n",
              " 'stitching': 642,\n",
              " 'complaint': 643,\n",
              " 'legs': 644,\n",
              " 'course': 645,\n",
              " 'attractive': 646,\n",
              " 'change': 647,\n",
              " 'totally': 648,\n",
              " 'friends': 649,\n",
              " 'styles': 650,\n",
              " 'products': 651,\n",
              " 'surprised': 652,\n",
              " 'everywhere': 653,\n",
              " 'beat': 654,\n",
              " 'doing': 655,\n",
              " 'w': 656,\n",
              " 'correct': 657,\n",
              " 'five': 658,\n",
              " 'replacement': 659,\n",
              " 'except': 660,\n",
              " 'nearly': 661,\n",
              " 'fairly': 662,\n",
              " 'china': 663,\n",
              " 'air': 664,\n",
              " 'dark': 665,\n",
              " 'pink': 666,\n",
              " 'spend': 667,\n",
              " 'version': 668,\n",
              " 'flexible': 669,\n",
              " 'choice': 670,\n",
              " 'difference': 671,\n",
              " 'footwear': 672,\n",
              " 'thinking': 673,\n",
              " 'night': 674,\n",
              " 'carry': 675,\n",
              " 'ugg': 676,\n",
              " 'fell': 677,\n",
              " 'fan': 678,\n",
              " 'higher': 679,\n",
              " 'plenty': 680,\n",
              " 'slide': 681,\n",
              " 'simply': 682,\n",
              " 'gone': 683,\n",
              " 'standing': 684,\n",
              " 'exact': 685,\n",
              " 'knew': 686,\n",
              " 'beach': 687,\n",
              " 'anywhere': 688,\n",
              " 'step': 689,\n",
              " 'smell': 690,\n",
              " 'anyway': 691,\n",
              " 'become': 692,\n",
              " 'immediately': 693,\n",
              " 'close': 694,\n",
              " 'ankles': 695,\n",
              " 'star': 696,\n",
              " 'th': 697,\n",
              " 'issues': 698,\n",
              " 'strong': 699,\n",
              " 'grip': 700,\n",
              " 'purse': 701,\n",
              " 'add': 702,\n",
              " 'dance': 703,\n",
              " 'dressy': 704,\n",
              " 'fun': 705,\n",
              " 'zipper': 706,\n",
              " 'compared': 707,\n",
              " 'twice': 708,\n",
              " 'fabric': 709,\n",
              " 'four': 710,\n",
              " 'sides': 711,\n",
              " 'suggest': 712,\n",
              " 'hoping': 713,\n",
              " 'huge': 714,\n",
              " 'mind': 715,\n",
              " 'keeps': 716,\n",
              " 'slippery': 717,\n",
              " 'particular': 718,\n",
              " 'tread': 719,\n",
              " 'flop': 720,\n",
              " 'name': 721,\n",
              " 'inch': 722,\n",
              " 'unless': 723,\n",
              " 'play': 724,\n",
              " 'trouble': 725,\n",
              " 'yes': 726,\n",
              " 'durability': 727,\n",
              " 'materials': 728,\n",
              " 'hour': 729,\n",
              " 'done': 730,\n",
              " 'supportive': 731,\n",
              " 'ran': 732,\n",
              " 'stuff': 733,\n",
              " 'lace': 734,\n",
              " 'alot': 735,\n",
              " 'continue': 736,\n",
              " 'green': 737,\n",
              " 'dirty': 738,\n",
              " 'skechers': 739,\n",
              " 'reasonable': 740,\n",
              " 'based': 741,\n",
              " 'steel': 742,\n",
              " 'theres': 743,\n",
              " 'reading': 744,\n",
              " 'shop': 745,\n",
              " 'forever': 746,\n",
              " 'returning': 747,\n",
              " 'match': 748,\n",
              " 'set': 749,\n",
              " 'feeling': 750,\n",
              " 'decent': 751,\n",
              " 'enjoy': 752,\n",
              " 'open': 753,\n",
              " 'takes': 754,\n",
              " 'tough': 755,\n",
              " 'offer': 756,\n",
              " 'gives': 757,\n",
              " 'adorable': 758,\n",
              " 'ecco': 759,\n",
              " 'future': 760,\n",
              " 'simple': 761,\n",
              " 'cause': 762,\n",
              " 'clogs': 763,\n",
              " 'footbed': 764,\n",
              " 'floors': 765,\n",
              " 'exchange': 766,\n",
              " 'forward': 767,\n",
              " 'impressed': 768,\n",
              " 'website': 769,\n",
              " 'poor': 770,\n",
              " 'keen': 771,\n",
              " 'tongue': 772,\n",
              " 'prefer': 773,\n",
              " 'anymore': 774,\n",
              " 'holding': 775,\n",
              " 'asics': 776,\n",
              " 'sold': 777,\n",
              " 'floor': 778,\n",
              " 'amount': 779,\n",
              " 'comfortably': 780,\n",
              " 'fitting': 781,\n",
              " 'walks': 782,\n",
              " 'pull': 783,\n",
              " 'reviewers': 784,\n",
              " 'inserts': 785,\n",
              " 'delivered': 786,\n",
              " 'against': 787,\n",
              " 'chance': 788,\n",
              " 'matter': 789,\n",
              " 'breakin': 790,\n",
              " 'refund': 791,\n",
              " 'sized': 792,\n",
              " 'along': 793,\n",
              " 'spring': 794,\n",
              " 'pocket': 795,\n",
              " 'hes': 796,\n",
              " 'considering': 797,\n",
              " 'flipflops': 798,\n",
              " 'important': 799,\n",
              " 'street': 800,\n",
              " 'falling': 801,\n",
              " 'taking': 802,\n",
              " 'putting': 803,\n",
              " 'minutes': 804,\n",
              " 'please': 805,\n",
              " 'description': 806,\n",
              " 'described': 807,\n",
              " 'mom': 808,\n",
              " 'amazoncom': 809,\n",
              " 'certainly': 810,\n",
              " 'em': 811,\n",
              " 'note': 812,\n",
              " 'site': 813,\n",
              " 'helps': 814,\n",
              " 'average': 815,\n",
              " 'tall': 816,\n",
              " 'wedding': 817,\n",
              " 'easier': 818,\n",
              " 'slipping': 819,\n",
              " 'priced': 820,\n",
              " 'lower': 821,\n",
              " 'spent': 822,\n",
              " 'incredibly': 823,\n",
              " 'excited': 824,\n",
              " 'remember': 825,\n",
              " 'sore': 826,\n",
              " 'polish': 827,\n",
              " 'built': 828,\n",
              " 'wash': 829,\n",
              " 'replaced': 830,\n",
              " 'calves': 831,\n",
              " 'birkenstock': 832,\n",
              " 'elastic': 833,\n",
              " 'leave': 834,\n",
              " 'ball': 835,\n",
              " 'smooth': 836,\n",
              " 'holds': 837,\n",
              " 'reef': 838,\n",
              " 'somewhat': 839,\n",
              " 'converse': 840,\n",
              " 'tie': 841,\n",
              " 'consider': 842,\n",
              " 'man': 843,\n",
              " 'rub': 844,\n",
              " 'appear': 845,\n",
              " 'dr': 846,\n",
              " 'knee': 847,\n",
              " 'starting': 848,\n",
              " 'holes': 849,\n",
              " 'leg': 850,\n",
              " 'called': 851,\n",
              " 'ask': 852,\n",
              " 'stop': 853,\n",
              " 'e': 854,\n",
              " 'together': 855,\n",
              " 'means': 856,\n",
              " 'rest': 857,\n",
              " 'arches': 858,\n",
              " 'world': 859,\n",
              " 'athletic': 860,\n",
              " 'constructed': 861,\n",
              " 'giving': 862,\n",
              " 'theyve': 863,\n",
              " 'near': 864,\n",
              " 'season': 865,\n",
              " 'keeping': 866,\n",
              " 'versatile': 867,\n",
              " 'truly': 868,\n",
              " 'items': 869,\n",
              " 'number': 870,\n",
              " 'oh': 871,\n",
              " 'navy': 872,\n",
              " 'glove': 873,\n",
              " 'piece': 874,\n",
              " 'nike': 875,\n",
              " 'd': 876,\n",
              " 'above': 877,\n",
              " 'fashion': 878,\n",
              " 'tan': 879,\n",
              " 'idea': 880,\n",
              " 'theyll': 881,\n",
              " 'wants': 882,\n",
              " 'moccasins': 883,\n",
              " 'surfaces': 884,\n",
              " 'designed': 885,\n",
              " 'merrell': 886,\n",
              " 'older': 887,\n",
              " 'inches': 888,\n",
              " 'notice': 889,\n",
              " 'women': 890,\n",
              " 'heal': 891,\n",
              " 'dog': 892,\n",
              " 'itself': 893,\n",
              " 'morning': 894,\n",
              " 'entire': 895,\n",
              " 'opinion': 896,\n",
              " 'dirt': 897,\n",
              " 'reebok': 898,\n",
              " 'nd': 899,\n",
              " 'mud': 900,\n",
              " 'cleats': 901,\n",
              " 'skin': 902,\n",
              " 'skirts': 903,\n",
              " 'stretched': 904,\n",
              " 'cushioned': 905,\n",
              " 'allow': 906,\n",
              " 'lighter': 907,\n",
              " 'mail': 908,\n",
              " 'birthday': 909,\n",
              " 'added': 910,\n",
              " 'roomy': 911,\n",
              " 'rough': 912,\n",
              " 'classy': 913,\n",
              " 'careful': 914,\n",
              " 'werent': 915,\n",
              " 'perhaps': 916,\n",
              " 'class': 917,\n",
              " 'uppers': 918,\n",
              " 'standard': 919,\n",
              " 'tired': 920,\n",
              " 'insert': 921,\n",
              " 'advertised': 922,\n",
              " 'six': 923,\n",
              " 'boat': 924,\n",
              " 'check': 925,\n",
              " 'asked': 926,\n",
              " 'receive': 927,\n",
              " 'unlike': 928,\n",
              " 'girl': 929,\n",
              " 'bed': 930,\n",
              " 'family': 931,\n",
              " 'turned': 932,\n",
              " 'soccer': 933,\n",
              " 'complaints': 934,\n",
              " 'metal': 935,\n",
              " 'thicker': 936,\n",
              " 'supposed': 937,\n",
              " 'ballet': 938,\n",
              " 'town': 939,\n",
              " 'throw': 940,\n",
              " 'okay': 941,\n",
              " 'patent': 942,\n",
              " 'timely': 943,\n",
              " 'penny': 944,\n",
              " 'hike': 945,\n",
              " 'lost': 946,\n",
              " 'painful': 947,\n",
              " 'credit': 948,\n",
              " 'mostly': 949,\n",
              " 'shine': 950,\n",
              " 'worried': 951,\n",
              " 'calf': 952,\n",
              " 'heard': 953,\n",
              " 'trail': 954,\n",
              " 'mother': 955,\n",
              " 'double': 956,\n",
              " 'baby': 957,\n",
              " 'thong': 958,\n",
              " 'including': 959,\n",
              " 'travel': 960,\n",
              " 'present': 961,\n",
              " 'concrete': 962,\n",
              " 'actual': 963,\n",
              " 'padded': 964,\n",
              " 'confortable': 965,\n",
              " 'slacks': 966,\n",
              " 'outfit': 967,\n",
              " 'foam': 968,\n",
              " 'changed': 969,\n",
              " 'despite': 970,\n",
              " 'shorts': 971,\n",
              " 'sweat': 972,\n",
              " 'happened': 973,\n",
              " 'mention': 974,\n",
              " 'originally': 975,\n",
              " 'cozy': 976,\n",
              " 'sock': 977,\n",
              " 'barely': 978,\n",
              " 'straight': 979,\n",
              " 'car': 980,\n",
              " 'finish': 981,\n",
              " 'stride': 982,\n",
              " 'rugged': 983,\n",
              " 'thrilled': 984,\n",
              " 'birkenstocks': 985,\n",
              " 'literally': 986,\n",
              " 'hand': 987,\n",
              " 'stick': 988,\n",
              " 'ready': 989,\n",
              " 'pockets': 990,\n",
              " 'boyfriend': 991,\n",
              " 'guy': 992,\n",
              " 'constantly': 993,\n",
              " 'special': 994,\n",
              " 'flats': 995,\n",
              " 'eventually': 996,\n",
              " 'hate': 997,\n",
              " 'appears': 998,\n",
              " 'outdoor': 999,\n",
              " 'gorgeous': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51e6QM5Bx66i",
        "outputId": "74367ab1-c15d-4630-d2ba-800be8d9e44c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#encoding of reviews (replace words in our reviews by integers)\n",
        "reviews_int = []\n",
        "for review in text:\n",
        "    r = [vocab_to_int[w] for w in review.split()]\n",
        "    reviews_int.append(r)\n",
        "print (reviews_int[0:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4927, 74, 217, 8, 434, 21, 25, 2, 16, 386, 5, 3391, 12, 1042, 5, 412, 2, 104, 38, 1, 15, 6, 46, 34, 3, 700, 5995, 195, 21, 700, 2, 511, 14, 1340, 13, 4, 2774, 111, 7, 727, 1, 314, 196, 300, 2670, 14, 204, 3, 13109, 523, 324, 2, 3704, 22, 4, 3504, 41, 3286, 68, 9182, 15, 12, 33, 540, 3, 126, 280, 19, 9182, 1201, 26185, 314, 5, 1227, 41, 821, 1104, 126, 280, 19, 108, 348, 200, 72, 9182, 16, 1, 4621, 8, 5143, 22, 148, 314, 305, 6, 9, 537, 13, 2, 45, 104, 109, 11, 15, 5, 331, 200, 14, 848, 54, 20, 188, 13, 715, 6, 53, 33, 206, 324, 402, 4927, 86, 96, 1268, 1280, 7, 68, 153, 217, 3, 74, 254, 18571, 22, 12, 11716, 223, 5503, 15, 2, 386, 2, 71, 286, 4, 29, 8, 12, 306, 5, 2429, 5504, 2, 39, 96, 5257, 17, 2, 38, 11, 7, 1, 1920, 1797, 169, 2, 221, 17, 4, 2062, 8, 15, 13, 536, 5, 166, 227, 2, 686, 101, 2, 24, 254, 18, 24, 4, 2775, 62, 446, 11, 3, 26186, 5652, 26187, 437, 1268, 25, 236, 1340, 4, 34, 46, 195, 2, 333, 19, 1, 3926, 1018, 26188, 39, 311, 8, 4, 2629, 174, 114, 1, 259, 280, 11, 16, 4, 34, 174, 255, 3, 1451, 10658, 167, 5, 585, 680, 8, 169, 49, 4928, 17, 1118, 837, 11, 94, 700, 12, 334, 271, 51, 28, 144, 2140, 3, 1, 38, 495, 4, 213, 2, 1224, 10, 491, 115, 7418, 20, 6, 9, 27, 167, 5, 834, 17, 7, 1527, 1163, 8, 13110, 800, 15, 2, 33, 89, 3, 2, 38, 12, 1268, 15, 261, 26, 2991, 261, 20, 5505, 2, 24, 95, 4, 13, 1, 5503, 8180, 15135, 41, 12, 1076, 195, 2, 24, 652, 19, 2, 39, 5, 32, 206, 5, 4, 13, 11, 5, 57, 1, 114, 2, 24, 112, 7, 152, 11, 495, 1381, 918, 2, 1477, 502, 10, 5, 449, 4, 128, 20, 30, 8, 102, 6, 9, 103, 3, 261, 1, 5377, 71, 585, 4, 135, 1289, 8, 46, 3, 9183, 2, 1372, 19, 1, 145, 181, 5, 117, 4, 1268, 25, 14, 5, 81, 241, 18, 17, 13, 536, 3079, 4, 6177, 22, 19, 25, 3, 177, 7, 120, 760, 2934, 143, 18, 26189, 21, 14, 237, 360, 69, 306, 15, 71, 4622, 111, 176, 2, 91, 11, 2, 24, 655, 3, 107, 5504, 74, 254, 11, 2, 15136, 107, 8, 1, 5504, 19, 2, 4713, 17, 176, 3, 1690, 732, 62, 10, 93, 102, 9184, 1480, 5504, 3, 743, 4, 93, 824, 5, 100, 11, 15, 16, 4, 1129, 26190, 106, 1087, 34, 3790, 7, 2190, 13111, 5504, 3, 167, 1133, 5, 584, 17, 1118, 837, 3620, 1, 174, 14, 479, 167, 5, 5378, 222, 2968, 2483, 3, 1, 144, 87, 4, 6849, 700, 106, 71, 8641, 837, 12, 26191, 87, 3832, 3392, 888, 22, 11, 3, 2, 64, 8642, 62, 717, 8181, 203, 364, 102, 1, 255, 87, 107, 1333, 68, 26192, 314, 17, 1, 219, 3, 2, 64, 3705, 5, 85, 26193, 28, 249, 2, 470, 39, 5, 82, 335, 5, 11, 5, 1491, 10, 58, 3538, 30, 22, 120, 25, 31, 208, 5, 519, 10, 664, 54, 74, 487, 2887, 384, 58, 3393, 1885, 242, 3180, 2, 16, 96, 1268, 68, 215, 4, 307, 13, 11, 7, 240, 380, 102, 3, 6, 77, 563, 4, 2444, 8, 33, 2, 1070, 305, 2, 4459, 1107, 2, 1910, 7419, 8182, 1268, 87, 18, 44, 34, 18572, 26194, 26195, 3, 666, 7123, 4, 128, 1192, 6, 366, 598, 5, 33, 20, 400, 177, 6, 257, 51, 26196, 2, 45, 209, 21, 14, 4, 34, 25, 7, 1280, 1268, 360, 26, 9857, 1399, 120, 389, 1018, 308, 2, 193, 2302, 2484, 85, 1510, 195, 2, 45, 1070, 19, 18, 45, 1822, 42, 294, 17, 389, 1018, 30, 18, 275, 17, 1180, 292, 1280, 15137, 9, 1893, 54, 8], [11, 53, 244, 31, 391, 2, 48, 11, 15, 70, 6, 312, 523, 179, 13, 1, 419, 3, 797, 6, 915, 42, 4, 29, 8, 15, 2, 247, 6, 45, 35, 839, 27, 50, 26, 7124, 5, 33, 1991, 1231, 11, 15, 53, 603, 69, 334, 6, 160, 4, 73, 157, 28, 2, 712, 254, 41, 341, 4, 218, 32, 269, 50, 31, 208, 5, 82, 335, 1486, 1387, 3, 112, 203, 82, 26, 72, 11, 15, 2, 639, 7, 311, 56, 804, 176, 6, 399, 2992, 3, 4821, 12, 334, 6, 94, 9, 4, 73, 261, 17, 1, 233, 28, 50, 31, 16, 4, 135, 84, 11, 230, 9, 26, 7, 31, 2, 16, 5, 1464, 6, 9, 3706, 3, 179, 20, 6, 9, 26, 225, 1, 391], [52, 21, 2146, 282, 564, 7420, 18573, 9, 42, 28, 421]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWUS-v_lyZ3B"
      },
      "source": [
        "labels = np.array(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y2AsEgiHd3D"
      },
      "source": [
        "#converting to binary class\n",
        "l=[]\n",
        "for i in labels:\n",
        "  if(i==0 or i==1 or i==2):\n",
        "    i=0\n",
        "  elif(i==3 or i==4 or i==5):\n",
        "    i=1\n",
        "  l.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjU1VYQgJzqu"
      },
      "source": [
        "label = np.array(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1bSxpAvy11B",
        "outputId": "8ac500bd-6b80-4868-cc0d-844652f69174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKUwmJB9zvV9"
      },
      "source": [
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iynsujgC0C6G"
      },
      "source": [
        "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
        "\n",
        "reviews = pad_input(reviews_int, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLI5_yvY0eoY",
        "outputId": "305b2757-4db5-4175-8620-39c6e9c159f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "reviews[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4927,    74,   217,     8,   434,    21,    25,     2,    16,\n",
              "         386,     5,  3391,    12,  1042,     5,   412,     2,   104,\n",
              "          38,     1,    15,     6,    46,    34,     3,   700,  5995,\n",
              "         195,    21,   700,     2,   511,    14,  1340,    13,     4,\n",
              "        2774,   111,     7,   727,     1,   314,   196,   300,  2670,\n",
              "          14,   204,     3, 13109,   523,   324,     2,  3704,    22,\n",
              "           4,  3504,    41,  3286,    68,  9182,    15,    12,    33,\n",
              "         540,     3,   126,   280,    19,  9182,  1201, 26185,   314,\n",
              "           5,  1227,    41,   821,  1104,   126,   280,    19,   108,\n",
              "         348,   200,    72,  9182,    16,     1,  4621,     8,  5143,\n",
              "          22,   148,   314,   305,     6,     9,   537,    13,     2,\n",
              "          45,   104,   109,    11,    15,     5,   331,   200,    14,\n",
              "         848,    54,    20,   188,    13,   715,     6,    53,    33,\n",
              "         206,   324,   402,  4927,    86,    96,  1268,  1280,     7,\n",
              "          68,   153,   217,     3,    74,   254, 18571,    22,    12,\n",
              "       11716,   223,  5503,    15,     2,   386,     2,    71,   286,\n",
              "           4,    29,     8,    12,   306,     5,  2429,  5504,     2,\n",
              "          39,    96,  5257,    17,     2,    38,    11,     7,     1,\n",
              "        1920,  1797,   169,     2,   221,    17,     4,  2062,     8,\n",
              "          15,    13,   536,     5,   166,   227,     2,   686,   101,\n",
              "           2,    24,   254,    18,    24,     4,  2775,    62,   446,\n",
              "          11,     3, 26186,  5652, 26187,   437,  1268,    25,   236,\n",
              "        1340,     4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK10eByN0iTZ"
      },
      "source": [
        "#80% train, 10% test & 10% validation\n",
        "split_frac = 0.8\n",
        "len_feat = len(reviews)\n",
        "train_x = reviews[0:int(split_frac*len_feat)]\n",
        "train_y = label[0:int(split_frac*len_feat)]\n",
        "remaining_x = reviews[int(split_frac*len_feat):]\n",
        "remaining_y = label[int(split_frac*len_feat):]\n",
        "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
        "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
        "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
        "test_y = remaining_y[int(len(remaining_y)*0.5):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWDjPD6C1TM1"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlFbIryf3z-k"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9beZSKf42k3",
        "outputId": "7b15a23b-731d-4cfb-9e3b-aed2d80c0e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([64, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,  1887,    15,   141],\n",
            "        [    0,     0,     0,  ...,     3,   271,   345],\n",
            "        [    0,     0,     0,  ...,    55,     4, 19417],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,     4,  2171,  4840],\n",
            "        [    0,     0,     0,  ...,   403,   316,    15],\n",
            "        [    0,     0,     0,  ...,    24,    23,  3876]])\n",
            "\n",
            "Sample label size:  torch.Size([64])\n",
            "Sample label: \n",
            " tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zHIIVDg4YCH"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEUiimnclYgl"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,dropout=drop_prob, batch_first=True)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3).to(device)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size).to(device)\n",
        "        self.sig = nn.Sigmoid().to(device)\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).normal_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).normal_().to(device))\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_IdgQuFRdKB"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljFijODkRojC"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers,dropout=drop_prob, batch_first=True)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3).to(device)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size).to(device)\n",
        "        self.sig = nn.Sigmoid().to(device)\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        gru_out, hidden = self.gru(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(gru_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6iBirSkRpDJ"
      },
      "source": [
        "#Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-VZcluhLBLp"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400 #\n",
        "batch_size = 64\n",
        "device = torch.device(\"cuda\")\n",
        "hidden_dim = 256\n",
        "n_layers = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LYT_aAYRgg4"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Em4lCwSLHAK"
      },
      "source": [
        "\n",
        "# training params\n",
        "def train(train_loader,valid_loader,epochs,model_type):\n",
        "  if model_type == \"LSTM\":\n",
        "    net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers).to(device)\n",
        "  elif (model_type == \"GRU\"):\n",
        "    net = GRU(vocab_size, output_size, embedding_dim, hidden_dim, n_layers).to(device)\n",
        "  lr=0.001\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "  counter = 0\n",
        "  print_every = 100\n",
        "  clip=5 # gradient clipping\n",
        "  net.train()\n",
        "  # train for some number of epochs\n",
        "  for e in range(epochs):\n",
        "      # initialize hidden state\n",
        "      h = net.init_hidden(batch_size)\n",
        "\n",
        "      # batch loop\n",
        "      for inputs, labels in train_loader:\n",
        "          counter += 1\n",
        "\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          # Creating new variables for the hidden state, otherwise\n",
        "          # we'd backprop through the entire training history\n",
        "          if model_type == \"GRU\":\n",
        "            h = h.data.to(device)\n",
        "          elif model_type == \"LSTM\":\n",
        "            h = tuple([each.data.to(device) for each in h])\n",
        "          # zero accumulated gradients\n",
        "          net.zero_grad()\n",
        "\n",
        "          # get the output from the model\n",
        "          inputs = inputs.type(torch.LongTensor).to(device)\n",
        "          output, h = net(inputs, h)\n",
        "          #print(output[0])\n",
        "          # calculate the loss and perform backprop\n",
        "          #print(\"shape of output\", output.shape)\n",
        "          #print(\"shape of labels\", labels.shape)\n",
        "          loss = criterion(output.squeeze(), labels.float())\n",
        "          loss.backward()\n",
        "          # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "          nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "          optimizer.step()\n",
        "\n",
        "          # loss stats\n",
        "          if counter % print_every == 0:\n",
        "              # Get validation loss\n",
        "              val_h = net.init_hidden(batch_size)\n",
        "              val_losses = []\n",
        "              net.eval()\n",
        "              for inputs, labels in valid_loader:\n",
        "\n",
        "                  # Creating new variables for the hidden state, otherwise\n",
        "                  # we'd backprop through the entire training history\n",
        "                  if model_type == \"GRU\":\n",
        "                    val_h = val_h.data.to(device)\n",
        "                  elif (model_type == \"LSTM\"):\n",
        "                    val_h = tuple([each.data.to(device) for each in val_h])\n",
        "\n",
        "                  inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                  inputs = inputs.type(torch.LongTensor).to(device)\n",
        "                  output, val_h = net(inputs, val_h)\n",
        "                  val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                  val_losses.append(val_loss.item())\n",
        "\n",
        "              net.train()\n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Model:{}...\".format(model_type),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "  return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JiM6N3-O0Fa"
      },
      "source": [
        "train_on_gpu = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qv9cWzjUEOj"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-ZNqtjkLZQZ"
      },
      "source": [
        "def test(net,test_loader,model_type):\n",
        "\n",
        "  # Get test data loss and accuracy\n",
        "  lr=0.001\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "  test_losses = [] # track loss\n",
        "  num_correct = 0\n",
        "\n",
        "  # init hidden state\n",
        "  h = net.init_hidden(batch_size)\n",
        "\n",
        "  net.eval()\n",
        "  # iterate over test data\n",
        "  for inputs, labels in test_loader:\n",
        "\n",
        "      # Creating new variables for the hidden state, otherwise\n",
        "      # we'd backprop through the entire training history\n",
        "      if model_type == \"GRU\":\n",
        "        h = h.data.to(device)\n",
        "      elif (model_type == \"LSTM\"):\n",
        "        h = tuple([each.data.to(device) for each in h])\n",
        "\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "      # get predicted outputs\n",
        "      inputs = inputs.type(torch.LongTensor).to(device)\n",
        "      output, h = net(inputs, h)\n",
        "      \n",
        "      # calculate loss\n",
        "      test_loss = criterion(output.squeeze(), labels.float())\n",
        "      test_losses.append(test_loss.item())\n",
        "      \n",
        "      # convert output probabilities to predicted class (0 or 1)\n",
        "      pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "      \n",
        "      # compare predictions to true label\n",
        "      correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "  # -- stats! -- ##\n",
        "  # avg test loss\n",
        "  print(\"Model: {}\".format(model_type))\n",
        "  print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "  # accuracy over all test data\n",
        "  test_acc = num_correct/len(test_loader.dataset)\n",
        "  print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZHrB-4L2-lQ"
      },
      "source": [
        "#Evaluating our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w8C-tZTOP4D",
        "outputId": "ee866210-bd79-4904-bc75-30ed4d65bf62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "lstm = train(train_loader,valid_loader,5,model_type=\"LSTM\")\n",
        "gru = train(train_loader,valid_loader,1,model_type=\"GRU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/5... Model:LSTM... Step: 100... Loss: 0.336632... Val Loss: 0.313262\n",
            "Epoch: 1/5... Model:LSTM... Step: 200... Loss: 0.235576... Val Loss: 0.235390\n",
            "Epoch: 1/5... Model:LSTM... Step: 300... Loss: 0.236306... Val Loss: 0.219724\n",
            "Epoch: 1/5... Model:LSTM... Step: 400... Loss: 0.243975... Val Loss: 0.211962\n",
            "Epoch: 1/5... Model:LSTM... Step: 500... Loss: 0.234142... Val Loss: 0.203561\n",
            "Epoch: 1/5... Model:LSTM... Step: 600... Loss: 0.191006... Val Loss: 0.201097\n",
            "Epoch: 1/5... Model:LSTM... Step: 700... Loss: 0.198479... Val Loss: 0.237612\n",
            "Epoch: 1/5... Model:LSTM... Step: 800... Loss: 0.200794... Val Loss: 0.197513\n",
            "Epoch: 1/5... Model:LSTM... Step: 900... Loss: 0.138439... Val Loss: 0.192942\n",
            "Epoch: 2/5... Model:LSTM... Step: 1000... Loss: 0.076694... Val Loss: 0.171586\n",
            "Epoch: 2/5... Model:LSTM... Step: 1100... Loss: 0.147182... Val Loss: 0.163820\n",
            "Epoch: 2/5... Model:LSTM... Step: 1200... Loss: 0.151632... Val Loss: 0.161675\n",
            "Epoch: 2/5... Model:LSTM... Step: 1300... Loss: 0.118741... Val Loss: 0.160947\n",
            "Epoch: 2/5... Model:LSTM... Step: 1400... Loss: 0.130022... Val Loss: 0.157802\n",
            "Epoch: 2/5... Model:LSTM... Step: 1500... Loss: 0.139838... Val Loss: 0.149888\n",
            "Epoch: 2/5... Model:LSTM... Step: 1600... Loss: 0.166701... Val Loss: 0.158044\n",
            "Epoch: 2/5... Model:LSTM... Step: 1700... Loss: 0.123692... Val Loss: 0.153743\n",
            "Epoch: 2/5... Model:LSTM... Step: 1800... Loss: 0.292243... Val Loss: 0.161533\n",
            "Epoch: 2/5... Model:LSTM... Step: 1900... Loss: 0.364518... Val Loss: 0.145615\n",
            "Epoch: 3/5... Model:LSTM... Step: 2000... Loss: 0.078482... Val Loss: 0.151595\n",
            "Epoch: 3/5... Model:LSTM... Step: 2100... Loss: 0.093398... Val Loss: 0.153241\n",
            "Epoch: 3/5... Model:LSTM... Step: 2200... Loss: 0.051980... Val Loss: 0.158011\n",
            "Epoch: 3/5... Model:LSTM... Step: 2300... Loss: 0.107540... Val Loss: 0.159897\n",
            "Epoch: 3/5... Model:LSTM... Step: 2400... Loss: 0.236647... Val Loss: 0.167488\n",
            "Epoch: 3/5... Model:LSTM... Step: 2500... Loss: 0.107261... Val Loss: 0.155872\n",
            "Epoch: 3/5... Model:LSTM... Step: 2600... Loss: 0.097970... Val Loss: 0.159150\n",
            "Epoch: 3/5... Model:LSTM... Step: 2700... Loss: 0.188950... Val Loss: 0.162950\n",
            "Epoch: 3/5... Model:LSTM... Step: 2800... Loss: 0.070314... Val Loss: 0.169051\n",
            "Epoch: 3/5... Model:LSTM... Step: 2900... Loss: 0.138062... Val Loss: 0.176014\n",
            "Epoch: 4/5... Model:LSTM... Step: 3000... Loss: 0.123961... Val Loss: 0.177420\n",
            "Epoch: 4/5... Model:LSTM... Step: 3100... Loss: 0.129195... Val Loss: 0.200558\n",
            "Epoch: 4/5... Model:LSTM... Step: 3200... Loss: 0.038100... Val Loss: 0.175377\n",
            "Epoch: 4/5... Model:LSTM... Step: 3300... Loss: 0.139979... Val Loss: 0.190031\n",
            "Epoch: 4/5... Model:LSTM... Step: 3400... Loss: 0.169517... Val Loss: 0.169413\n",
            "Epoch: 4/5... Model:LSTM... Step: 3500... Loss: 0.141543... Val Loss: 0.170138\n",
            "Epoch: 4/5... Model:LSTM... Step: 3600... Loss: 0.070509... Val Loss: 0.172190\n",
            "Epoch: 4/5... Model:LSTM... Step: 3700... Loss: 0.075166... Val Loss: 0.168227\n",
            "Epoch: 4/5... Model:LSTM... Step: 3800... Loss: 0.078796... Val Loss: 0.168536\n",
            "Epoch: 4/5... Model:LSTM... Step: 3900... Loss: 0.063467... Val Loss: 0.170523\n",
            "Epoch: 5/5... Model:LSTM... Step: 4000... Loss: 0.031780... Val Loss: 0.188903\n",
            "Epoch: 5/5... Model:LSTM... Step: 4100... Loss: 0.043289... Val Loss: 0.245377\n",
            "Epoch: 5/5... Model:LSTM... Step: 4200... Loss: 0.048604... Val Loss: 0.228748\n",
            "Epoch: 5/5... Model:LSTM... Step: 4300... Loss: 0.018346... Val Loss: 0.201570\n",
            "Epoch: 5/5... Model:LSTM... Step: 4400... Loss: 0.088969... Val Loss: 0.219431\n",
            "Epoch: 5/5... Model:LSTM... Step: 4500... Loss: 0.067888... Val Loss: 0.213822\n",
            "Epoch: 5/5... Model:LSTM... Step: 4600... Loss: 0.029429... Val Loss: 0.209240\n",
            "Epoch: 5/5... Model:LSTM... Step: 4700... Loss: 0.073053... Val Loss: 0.208099\n",
            "Epoch: 5/5... Model:LSTM... Step: 4800... Loss: 0.036001... Val Loss: 0.206227\n",
            "Epoch: 5/5... Model:LSTM... Step: 4900... Loss: 0.020536... Val Loss: 0.217643\n",
            "Epoch: 1/1... Model:GRU... Step: 100... Loss: 0.294786... Val Loss: 0.309728\n",
            "Epoch: 1/1... Model:GRU... Step: 200... Loss: 0.212687... Val Loss: 0.229514\n",
            "Epoch: 1/1... Model:GRU... Step: 300... Loss: 0.156035... Val Loss: 0.178832\n",
            "Epoch: 1/1... Model:GRU... Step: 400... Loss: 0.269708... Val Loss: 0.166360\n",
            "Epoch: 1/1... Model:GRU... Step: 500... Loss: 0.167439... Val Loss: 0.163574\n",
            "Epoch: 1/1... Model:GRU... Step: 600... Loss: 0.177211... Val Loss: 0.155639\n",
            "Epoch: 1/1... Model:GRU... Step: 700... Loss: 0.219579... Val Loss: 0.153870\n",
            "Epoch: 1/1... Model:GRU... Step: 800... Loss: 0.139773... Val Loss: 0.149318\n",
            "Epoch: 1/1... Model:GRU... Step: 900... Loss: 0.158969... Val Loss: 0.144819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ7O5VuIU4qx",
        "outputId": "4e647a17-5c9b-461d-fe59-40d685b14111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "test(lstm,test_loader,model_type =\"LSTM\")\n",
        "test(gru,test_loader,model_type=\"GRU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: LSTM\n",
            "Test loss: 0.295\n",
            "Test accuracy: 0.924\n",
            "Model: GRU\n",
            "Test loss: 0.175\n",
            "Test accuracy: 0.926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zktz-c4a2u4K"
      },
      "source": [
        "#Predicting the sentiment of user data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghc3gV_CqNf7"
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower() # lowercase\n",
        "    # get rid of punctuation\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "\n",
        "    # splitting by spaces\n",
        "    test_words = test_text.split()\n",
        "\n",
        "    # tokens\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "\n",
        "    return test_ints\n",
        "\n",
        "\n",
        "def predict(net, test_review, sequence_length=200):\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    # tokenize review\n",
        "    test_ints = tokenize_review(test_review)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_input(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    feature_tensor = feature_tensor.to(device)\n",
        "    \n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output) \n",
        "    # printing output value, before rounding\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "    \n",
        "    # print custom response\n",
        "    if(pred.item()==1):\n",
        "        print(\"Positive review detected!\")\n",
        "    else:\n",
        "        print(\"Negative review detected.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxPei2VT2s2Q",
        "outputId": "082b2ab8-f828-45ec-99b3-df6e7d3b368a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "test_review = 'This product was  good. I love it.'\n",
        "seq_length=200 # good to use the length that was trained on\n",
        "model_type= input(\"Type of model you want to test on \")\n",
        "if model_type == \"LSTM\":\n",
        "  net = lstm\n",
        "elif (model_type == \"GRU\"):\n",
        "  net = gru\n",
        "\n",
        "predict(net, test_review, seq_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of model you want to test on GRU\n",
            "Prediction value, pre-rounding: 0.997184\n",
            "Positive review detected!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjbhpwfC5OEk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}